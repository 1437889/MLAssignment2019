{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(v):\n",
    "    v = np.array(v).astype(np.float)\n",
    "    v = v.reshape(1, -1)\n",
    "    return(v.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileProcessor(F, featName, trainingSize):\n",
    "    x_train = []   #Training Data\n",
    "    x_test = []    #Testing Data \n",
    "    y_train = []   #Training Classifiers\n",
    "    y_test = []    #Testing Classifiers\n",
    "    first = True\n",
    "    file1 = True\n",
    "    for file in F:    #F is a vector of file names\n",
    "        x = []\n",
    "        y = []\n",
    "        tree = ET.parse(file) #Create XML file tree\n",
    "        root = tree.getroot() #Get root of tree... In our case root is 'feature_vector_file'\n",
    "        for child in root:    #Loop through subelements of root\n",
    "            if child.tag == 'data_set':      #if subelement is 'data_set', perform:\n",
    "                fileID = child.find('data_set_id').text   #'data_set_id' contains the classifer (i.e the composer name)\n",
    "                composerName = fileID.split('/')[-2]      #extract composer name from 'data_set_id'\n",
    "                if composerName == 'Beethoven':\n",
    "                    y.append(0)\n",
    "                elif composerName == 'Schubert':\n",
    "                    y.append(1)\n",
    "                elif composerName == 'Mozart':\n",
    "                    y.append(2)\n",
    "                else:\n",
    "                    y.append(3)\n",
    "                for feature in child.iter('feature'):  #Loop through 'data_set' subelement for features\n",
    "                    if feature.find('name').text in [featName]:#['Power Spectrum Overall Average', 'Spectral Flux Overall Average', 'Beat Histogram Overall Average', 'MFCC Overall Average']:\n",
    "                        v = []                                #create empty array to contain feature values\n",
    "                        vals = feature.findall('v')           #extract values of feature\n",
    "                        for i in vals:                    #loop through extracted values\n",
    "                            v.append(i.text)                  #append text of values to values array\n",
    "                        v = standardize(v)\n",
    "                        if first == True:\n",
    "                            x = v\n",
    "                            first = False\n",
    "                        else:\n",
    "                            x = np.c_[x, v]\n",
    "#         print(x.shape)\n",
    "        first = True\n",
    "        y = np.array(y)\n",
    "        x_trainC, x_testC, y_trainC, y_testC = train_test_split(x.T, y, test_size=trainingSize)\n",
    "        if file1 == True:\n",
    "            x_train = x_trainC.T\n",
    "#             print(x_train)\n",
    "            x_test = x_testC.T\n",
    "            y_train = y_trainC\n",
    "#             print(y_train)\n",
    "            y_test = y_testC\n",
    "            file1 = False\n",
    "        else:\n",
    "            x_train = np.c_[x_train, x_trainC.T]    \n",
    "            x_test = np.c_[x_test, x_testC.T]  \n",
    "            y_train = np.concatenate((y_train, y_trainC))\n",
    "#             print(y_train)\n",
    "            y_test = np.concatenate((y_test, y_testC)) \n",
    "    \n",
    "#     y = np.array(y)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x.T, y, test_size=trainingSize)\n",
    "#     print(len(x_train))\n",
    "    scaleX = StandardScaler()\n",
    "    x_train = scaleX.fit_transform(x_train.T)\n",
    "    x_test = scaleX.transform(x_test.T)\n",
    "    \n",
    "    return(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileProcessor2(F, featName, trainingSize):\n",
    "    x_train = []   #Training Data\n",
    "    x_test = []    #Testing Data \n",
    "    y_train = []   #Training Classifiers\n",
    "    y_test = []    #Testing Classifiers\n",
    "    file1 = True\n",
    "    for file in F:    #F is a vector of file names\n",
    "        x = []\n",
    "        y = []\n",
    "        first2 = True\n",
    "        tree = ET.parse(file) #Create XML file tree\n",
    "        root = tree.getroot() #Get root of tree... In our case root is 'feature_vector_file'\n",
    "        for child in root:    #Loop through subelements of root\n",
    "            xx = []\n",
    "            first = True\n",
    "            if child.tag == 'data_set':      #if subelement is 'data_set', perform:\n",
    "                fileID = child.find('data_set_id').text   #'data_set_id' contains the classifer (i.e the composer name)\n",
    "                composerName = fileID.split('/')[-2]      #extract composer name from 'data_set_id'\n",
    "                if composerName == 'Beethoven':\n",
    "                    y.append(0)\n",
    "                elif composerName == 'Schubert':\n",
    "                    y.append(1)\n",
    "                elif composerName == 'Mozart':\n",
    "                    y.append(2)\n",
    "                else:\n",
    "                    y.append(3)\n",
    "                for feature in child.iter('feature'):  #Loop through 'data_set' subelement for features\n",
    "                    if feature.find('name').text in featName:#['Power Spectrum Overall Average', 'Spectral Flux Overall Average', 'Beat Histogram Overall Average', 'MFCC Overall Average']:\n",
    "                        v = []                                #create empty array to contain feature values\n",
    "#                         print('here')\n",
    "                        vals = feature.findall('v')           #extract values of feature\n",
    "                        for i in vals:                    #loop through extracted values\n",
    "                            v.append(i.text)                  #append text of values to values array\n",
    "                        v = standardize(v)\n",
    "#                         print(v)\n",
    "                        if first == True:\n",
    "                            xx = v\n",
    "                            first = False\n",
    "                        else:\n",
    "                            xx = np.r_[xx, v]\n",
    "#                         print('feature done')\n",
    "#                 print('song done')\n",
    "                if first2 == True:\n",
    "                    x = xx\n",
    "                    first2 = False\n",
    "                else:\n",
    "#                     print(x)\n",
    "#                     print(xx)\n",
    "                    x = np.c_[x, xx]\n",
    "#             print(x)\n",
    "        first = True\n",
    "        y = np.array(y)\n",
    "        x_trainC, x_testC, y_trainC, y_testC = train_test_split(x.T, y, test_size=trainingSize)\n",
    "        if file1 == True:\n",
    "            x_train = x_trainC.T\n",
    "#             print(x_train)\n",
    "            x_test = x_testC.T\n",
    "            y_train = y_trainC\n",
    "#             print(y_train)\n",
    "            y_test = y_testC\n",
    "            file1 = False\n",
    "        else:\n",
    "            x_train = np.c_[x_train, x_trainC.T]    \n",
    "            x_test = np.c_[x_test, x_testC.T]  \n",
    "            y_train = np.concatenate((y_train, y_trainC))\n",
    "#             print(y_train)\n",
    "            y_test = np.concatenate((y_test, y_testC)) \n",
    "    \n",
    "#     y = np.array(y)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x.T, y, test_size=trainingSize)\n",
    "#     print(len(x_train))\n",
    "    scaleX = StandardScaler()\n",
    "    x_train = scaleX.fit_transform(x_train.T)\n",
    "    x_test = scaleX.transform(x_test.T)\n",
    "    \n",
    "    return(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(data):\n",
    "    x_train = data[0]\n",
    "    x_test = data[1]\n",
    "    y_train = data[2]\n",
    "    y_test = data[3]\n",
    "    \n",
    "    epsilon = (1e-9)*np.var(x_train, axis=0).max()\n",
    "    \n",
    "    featureLength = x_train.shape[1]\n",
    "    composerSize = len(np.unique(y_train))\n",
    "    mean = np.zeros((composerSize, featureLength))\n",
    "    var = np.zeros((composerSize, featureLength))\n",
    "    \n",
    "    composer_count = np.zeros(composerSize, dtype=np.float64)\n",
    "    composer_prior = np.zeros(len(np.unique(y_train)),dtype=np.float64)\n",
    "    \n",
    "    composers = np.unique(y_train)\n",
    "    \n",
    "    for y_i in composers:\n",
    "        i = np.searchsorted(composers, y_i)\n",
    "        composerData = x_train[y_train == y_i, :]\n",
    "\n",
    "        N_i = composerData.shape[0]\n",
    "\n",
    "        new_var = np.var(composerData, axis=0)\n",
    "        new_mean = np.mean(composerData, axis=0)\n",
    "        \n",
    "        mean[i, :] = new_mean\n",
    "        var[i, :] = new_var\n",
    "        composer_count[i] += N_i\n",
    "        \n",
    "    var[:, :] += epsilon\n",
    "    \n",
    "    composer_prior = composer_count/sum(composer_count)\n",
    "    return(composers, composer_prior, var, mean, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(data):\n",
    "    composers = data[0]\n",
    "    composer_prior = data[1]\n",
    "    var = data[2]\n",
    "    mean = data[3]\n",
    "    X = data[4]\n",
    "    y_test = data[5]\n",
    "    likelihood = []\n",
    "    for i in range(np.size(composers)):\n",
    "        joint = np.log(composer_prior[i])\n",
    "        nij = - 0.5 * np.sum(np.log(2. * np.pi * var[i, :]))\n",
    "        nij -= 0.5 * np.sum(((X - mean[i, :]) ** 2) /\n",
    "                             (var[i, :]), 1)\n",
    "        likelihood.append(joint + nij)\n",
    "\n",
    "    likelihood = np.array(likelihood).T\n",
    "    y_pred = composers[np.argmax(likelihood, axis=1)].T\n",
    "    \n",
    "#     print('Results  |  True')\n",
    "#     for i in range(len(y_pred)):\n",
    "#         print(y_pred[i], y_test[i])\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Spectrum Overall Average\n",
      "[[6 0 4 0]\n",
      " [5 1 3 1]\n",
      " [0 0 7 0]\n",
      " [4 2 6 4]]\n",
      "\n",
      "Spectral Flux Overall Average\n",
      "[[4 2 4 0]\n",
      " [1 0 6 3]\n",
      " [0 3 4 0]\n",
      " [2 1 7 6]]\n",
      "\n",
      "Beat Histogram Overall Average\n",
      "[[4 1 3 2]\n",
      " [3 1 3 3]\n",
      " [3 0 3 1]\n",
      " [5 1 2 8]]\n",
      "\n",
      "MFCC Overall Average\n",
      "[[4 4 1 1]\n",
      " [1 7 1 1]\n",
      " [1 0 5 1]\n",
      " [2 7 2 5]]\n",
      "\n",
      "All Features\n",
      "[[5 2 2 1]\n",
      " [6 1 2 1]\n",
      " [0 1 5 1]\n",
      " [6 1 2 7]]\n"
     ]
    }
   ],
   "source": [
    "v = ['beethovenValues.xml', 'schubertValues.xml', 'mozartValues.xml', 'chopinValues.xml']\n",
    "features = ['Power Spectrum Overall Average', 'Spectral Flux Overall Average', 'Beat Histogram Overall Average', 'MFCC Overall Average']\n",
    "for i in range(len(features)):\n",
    "    print(features[i])\n",
    "    d = fileProcessor(v, features[i], 0.33)\n",
    "    n = NaiveBayes(d)\n",
    "    testing(n)\n",
    "    print('')\n",
    "print('All Features')\n",
    "d = fileProcessor2(v, features, 0.33)\n",
    "n = NaiveBayes(d)\n",
    "testing(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
