{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat Histogram preprocessing\n",
    "\n",
    "Change the `'Beat Histogram Overall Standard Deviation'` feature from a list to a single value by taking the average of values, Then normalize this value, using a max of 1.5\n",
    "\n",
    "## Notes\n",
    "\n",
    "The validity of this transformation needs to be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessBeatHist(vals):\n",
    "    avg = (sum(vals) / len(vals))\n",
    "    return (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fileProcessor(F, trainingSize):\n",
    "    x_train = []   #Training Data\n",
    "    x_test = []    #Testing Data \n",
    "    y_train = []   #Training Classifiers\n",
    "    y_test = []    #Testing Classifiers\n",
    "    for file in F:    #F is a vector of file names\n",
    "        x = [] #data\n",
    "        y = [] #classifier\n",
    "        tree = ET.parse(file) #Create XML file tree\n",
    "        root = tree.getroot() #Get root of tree... In our case root is 'feature_vector_file'\n",
    "        for child in root:    #Loop through subelements of root\n",
    "            if child.tag == 'data_set':      #if subelement is 'data_set', perform:\n",
    "                fileID = child.find('data_set_id').text   #'data_set_id' contains the classifer (i.e the composer name)\n",
    "                composerName = fileID.split('/')[-2]      #extract composer name from 'data_set_id'\n",
    "                y.append(composerName)                    #append composer name to classifier vector\n",
    "                FeatureList = []                          #empty array for List of Features, Features have a name and set of values\n",
    "                for feature in child.iter('feature'):  #Loop through 'data_set' subelement for features\n",
    "                    v = []                                #create empty array to contain feature values\n",
    "                    name = feature.find('name').text      #extract name of feature\n",
    "                    vals = feature.findall('v')           #extract values of feature\n",
    "                    for i in vals:                    #loop through extracted values\n",
    "                        v.append(float(str(i.text).replace('E', 'e')))                  #append text of values to values array\n",
    "                        #preprocessing.scale(v)             normalisation\n",
    "#                         print(name)\n",
    "                    if name == ('Beat Histogram Overall Average'):\n",
    "#                         print(v)\n",
    "                        v = preProcessBeatHist(v)\n",
    "                    feat = [name, v]                  #join feature name and values\n",
    "                    FeatureList.append(feat)          #append feature name and values to List of Features\n",
    "                x.append(FeatureList)           #append List of Features for midi file being analysed to the array of data\n",
    "        x_trainC, x_testC, y_trainC, y_testC = train_test_split(x, y,test_size=trainingSize) #split data into training and testing data\n",
    "        #the following is done to ensure that each composer has an equal ratio of music split\n",
    "        #into training and testing data... although the specific songs by each composer are chosen randomly\n",
    "        #each composer has an equal ratio of their music split into testing and training data\n",
    "        x_train = x_train + x_trainC    \n",
    "        x_test = x_test + x_testC\n",
    "        y_train = y_train + y_trainC\n",
    "        y_test = y_test + y_testC\n",
    "#     print(y_train)\n",
    "    return [y_train, x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ['beethovenValues.xml', 'chopinValues.xml']\n",
    "[y,x] = fileProcessor(v, 0.33)\n",
    "# preProcessBeatHist(x[0][2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat Histogram Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normPriorBeatHist(data):\n",
    "    vals = []\n",
    "    for i in range(len(data)):\n",
    "        vals.append(data[i][6][1])\n",
    "    # Fit a normal distribution to the data:\n",
    "    [mu, std] = norm.fit(vals)\n",
    "    return [mu, std]\n",
    "#     plt.hist(vals, bins=25)\n",
    "#     # Plot the PDF.\n",
    "#     xmin, xmax = plt.xlim()\n",
    "#     x = np.linspace(xmin, xmax, 100)\n",
    "#     p = norm.pdf(x, mu, std)\n",
    "#     plt.plot(x, p, 'k', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normBeatHistLikelihood(data, labels, composer):\n",
    "    h = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == composer:\n",
    "            h.append(data[i][6][1])\n",
    "    [mu, std] = norm.fit(h)\n",
    "    return [mu, std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ['beethovenValues.xml', 'chopinValues.xml']\n",
    "[y,x] = fileProcessor(v, 0.33)\n",
    "normBeatHistLikelihood(x,y,y[0])\n",
    "[m, s] = normPriorBeatHist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBeatLikelihood(value, model):\n",
    "#     return norm(value[6][1], model[1]).cdf(model[0] + (0.125*model[1])) - norm(model[0], model[1]).cdf(value[6][1] - (0.125*model[1]))\n",
    "    return norm(model[0], model[1]).pdf(value[6][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0874166105637393"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateBeatLikelihood(x[0], [m,s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
