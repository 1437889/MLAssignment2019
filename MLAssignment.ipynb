{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileProcessor(F, featName, trainingSize):\n",
    "    x_train = []   #Training Data\n",
    "    x_test = []    #Testing Data \n",
    "    y_train = []   #Training Classifiers\n",
    "    y_test = []    #Testing Classifiers\n",
    "    file1 = True\n",
    "    for file in F:    #F is a vector of file names\n",
    "        x = []\n",
    "        y = []\n",
    "        print(file)\n",
    "        tree = ET.parse(file) #Create XML file tree\n",
    "        root = tree.getroot() #Get root of tree... In our case root is 'feature_vector_file'\n",
    "        for child in root:    #Loop through subelements of root\n",
    "            xx = []\n",
    "            if child.tag == 'data_set':      #if subelement is 'data_set', perform:\n",
    "                fileID = child.find('data_set_id').text   #'data_set_id' contains the classifer (i.e the composer name)\n",
    "                composerName = fileID.split('/')[-2]      #extract composer name from 'data_set_id'\n",
    "                composerID = 0\n",
    "                if composerName == 'Beethoven':\n",
    "#                     y.append(0)\n",
    "                    composerID = 0\n",
    "                elif composerName == 'Schubert':\n",
    "#                     y.append(1)\n",
    "                    composerID = 1\n",
    "                elif composerName == 'Mozart':\n",
    "#                     y.append(2)\n",
    "                    composerID = 2\n",
    "                else:\n",
    "#                     y.append(3)\n",
    "                    composerID = 3\n",
    "                first = True\n",
    "                for section in child.iter('section'):  #Loop through 'data_set' subelement for features\n",
    "                    if first == False:\n",
    "                        y.append(composerID)\n",
    "                        for feature in section.iter('feature'):\n",
    "                            if feature.find('name').text in featName:#['Power Spectrum Overall Average', 'Spectral Flux Overall Average', 'Beat Histogram Overall Average', 'MFCC Overall Average']:\n",
    "                                v = []                                #create empty array to contain feature values\n",
    "                                vals = feature.findall('v')           #extract values of feature\n",
    "                                for i in vals:                    #loop through extracted values\n",
    "                                    v.append(i.text)                  #append text of values to values array\n",
    "                                v = np.array(v).astype(np.float)\n",
    "                                v = v.reshape(1, -1)\n",
    "                                v = v.T\n",
    "                                if len(xx) == 0:\n",
    "                                    xx = v\n",
    "                                else:\n",
    "                                    if len(featName) == 1:\n",
    "                                        xx = np.c_[x, v]\n",
    "                                    else:\n",
    "                                        xx = np.r_[xx, v]\n",
    "                        if len(x) == 0:\n",
    "                            x = xx\n",
    "                        else:\n",
    "                            x = np.c_[x, xx]\n",
    "                    first = False\n",
    "        y = np.array(y)\n",
    "        x_trainC, x_testC, y_trainC, y_testC = train_test_split(x.T, y, test_size=trainingSize)\n",
    "        if file1 == True:\n",
    "            x_train = x_trainC.T\n",
    "            x_test = x_testC.T\n",
    "            y_train = y_trainC\n",
    "            y_test = y_testC\n",
    "            file1 = False\n",
    "        else:\n",
    "            x_train = np.c_[x_train, x_trainC.T]    \n",
    "            x_test = np.c_[x_test, x_testC.T]  \n",
    "            y_train = np.concatenate((y_train, y_trainC))\n",
    "            y_test = np.concatenate((y_test, y_testC)) \n",
    "    \n",
    "    scaleX = StandardScaler()\n",
    "    x_train = scaleX.fit_transform(x_train.T)\n",
    "    x_test = scaleX.transform(x_test.T)\n",
    "    \n",
    "    return(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(data):\n",
    "    #init\n",
    "    x_train = data[0]\n",
    "    x_test = data[1]\n",
    "    y_train = data[2]\n",
    "    y_test = data[3]\n",
    "    learningRate = 0.1\n",
    "    iter_n = 100\n",
    "\n",
    "    #fit\n",
    "    x_train = np.insert(x_train, 0, 1, axis=1)\n",
    "    weights = []\n",
    "    m = x_train.shape[0]\n",
    "    for i in np.unique(y_train):\n",
    "        y_copy = np.where(y_train == i, 1, 0)\n",
    "        w = np.ones(x_train.shape[1])\n",
    "        for _ in range(iter_n):\n",
    "            output = x_train.dot(w)\n",
    "            errors = y_copy - 1 / (1 + np.exp(-output))\n",
    "            w += learningRate / m * errors.dot(x_train)\n",
    "        weights.append((w, i))\n",
    "\n",
    "    pred = [max((i.dot(w), c) for w, c in weights)[1] for i in np.insert(x_test, 0, 1, axis=1)]\n",
    "    return(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(data):\n",
    "    x_train = data[0]\n",
    "    x_test = data[1]\n",
    "    y_train = data[2]\n",
    "    y_test = data[3]\n",
    "    \n",
    "    epsilon = (1e-9)*np.var(x_train, axis=0).max()\n",
    "    \n",
    "    featureLength = x_train.shape[1]\n",
    "    composerSize = len(np.unique(y_train))\n",
    "    mean = np.zeros((composerSize, featureLength))\n",
    "    var = np.zeros((composerSize, featureLength))\n",
    "    \n",
    "    composer_count = np.zeros(composerSize, dtype=np.float64)\n",
    "    composer_prior = np.zeros(len(np.unique(y_train)),dtype=np.float64)\n",
    "    \n",
    "    composers = np.unique(y_train)\n",
    "    \n",
    "    for y_i in composers:\n",
    "        i = np.searchsorted(composers, y_i)\n",
    "        composerData = x_train[y_train == y_i, :]\n",
    "\n",
    "        N_i = composerData.shape[0]\n",
    "\n",
    "        new_var = np.var(composerData, axis=0)\n",
    "        new_mean = np.mean(composerData, axis=0)\n",
    "        \n",
    "        mean[i, :] = new_mean\n",
    "        var[i, :] = new_var\n",
    "        composer_count[i] += N_i\n",
    "        \n",
    "    var[:, :] += epsilon\n",
    "    \n",
    "    composer_prior = composer_count/sum(composer_count)\n",
    "    \n",
    "    likelihood = []\n",
    "    for i in range(np.size(composers)):\n",
    "        joint = np.log(composer_prior[i])\n",
    "        nij = - 0.5 * np.sum(np.log(2. * np.pi * var[i, :]))\n",
    "        nij -= 0.5 * np.sum(((x_test - mean[i, :]) ** 2) /\n",
    "                             (var[i, :]), 1)\n",
    "        likelihood.append(joint + nij)\n",
    "\n",
    "    likelihood = np.array(likelihood).T\n",
    "    y_pred = composers[np.argmax(likelihood, axis=1)].T\n",
    "    \n",
    "    \n",
    "    return(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data):\n",
    "    y_test = data[0]\n",
    "    y_pred = data[1]\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "#     print(cm)\n",
    "#     print(accuracy_score(y_test, y_pred, normalize=True)*100)\n",
    "    return(cm, accuracy_score(y_test, y_pred, normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printM(matrix):\n",
    "    targets = ['Beethoven', 'Schubert', 'Mozart', 'Chopin']\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Pastel1)\n",
    "    plt.title('Confusion matrix')\n",
    "    tick_marks = np.arange(len(targets))\n",
    "    plt.xticks(tick_marks, targets, rotation=90)\n",
    "    plt.yticks(tick_marks, targets)\n",
    "    plt.tight_layout()\n",
    "    width, height = matrix.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(matrix[x][y]), xy=(y,x),\n",
    "                        horizontalalignment = 'center',\n",
    "                        verticalalignment= 'center')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ['beethovenValues.xml', 'schubertValues.xml', 'mozartValues.xml', 'chopinValues.xml']\n",
    "features = ['Spectral Flux', 'Compactness', 'Spectral Variability', 'Root Mean Square', 'Zero Crossings', 'Strongest Frequency Via Zero Crossings', 'Strongest Frequency Via Spectral Centroid', 'Strongest Frequency Via FFT Maximum', 'MFCC', 'LPC', 'Method of Moments', 'Partial Based Spectral Centroid', 'Partial Based Spectral Flux', 'Peak Based Spectral Smoothness', 'Relative Difference Function', 'Area Method of Moments', 'Area Method of Moments of MFCCs']\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(features[i])\n",
    "    d = fileProcessor(v, [features[i]], 0.33)\n",
    "    print('Naive Bayes')\n",
    "    n = NaiveBayes(d)\n",
    "    printM(plot(n)[0])\n",
    "    print('Accuracy = '+str(plot(n)[1]))\n",
    "    print('')\n",
    "    print('Logistic Regression')\n",
    "    l = LogisticRegression(d)\n",
    "    printM(plot(l)[0])\n",
    "    print('Accuracy = '+str(plot(l)[1]))\n",
    "    print('')\n",
    "    print('')\n",
    "print('All Features')\n",
    "d = fileProcessor(v, features, 0.33)\n",
    "print('Naive Bayes')\n",
    "n = NaiveBayes(d)\n",
    "printM(plot(n)[0])\n",
    "print('Accuracy = '+str(plot(n)[1]))\n",
    "print('Logistic Regression')\n",
    "l = LogisticRegression(d)\n",
    "printM(plot(l)[0])\n",
    "print('Accuracy = '+str(plot(l)[1]))\n",
    "print('')\n",
    "\n",
    "nm = 1000\n",
    "print('Performing Methods Over '+str(nm)+' Iterations')\n",
    "for i in range(len(features)):\n",
    "    print(features[i])\n",
    "    sumN = 0\n",
    "    sumL = 0\n",
    "    for j in range(nm):\n",
    "        d1 = fileProcessor(v, [features[i]], 0.33)\n",
    "        n1 = NaiveBayes(d1)\n",
    "        sumN = sumN + plot(n1)[1]\n",
    "        l1 = LogisticRegression(d1)\n",
    "        sumL = sumL + plot(l1)[1]\n",
    "    print('Naive Bayes Accuracy')\n",
    "    print(sumN/nm)\n",
    "    print('')\n",
    "    print('Logistic Regression Accuracy')\n",
    "    print(sumL/nm)\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "# print('Over All Features')\n",
    "# sumN = 0\n",
    "# sumL = 0\n",
    "# for k in range(nm):\n",
    "#     dj = fileProcessor(v, features, 0.33)\n",
    "#     nj = NaiveBayes(dj)\n",
    "#     sumN = sumN + plot(nj)[1]\n",
    "#     lj = LogisticRegression(dj)\n",
    "#     sumL = sumL + plot(lj)[1]\n",
    "# print('Naive Bayes Accuracy')\n",
    "# print(sumN/nm)\n",
    "# print('')\n",
    "# print('Logistic Regression Accuracy')\n",
    "# print(sumL/nm)\n",
    "# print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
