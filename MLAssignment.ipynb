{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCCnormalizer(mfcc):\n",
    "    mfcc = np.array(mfcc).astype(np.float)\n",
    "    eps = 2**-30\n",
    "    mean = np.mean(mfcc)\n",
    "    mean_vec = np.tile(mean, (len(mfcc), 1)).T\n",
    "    mean_subtracted = mfcc - mean_vec\n",
    "    stdev = np.std(mfcc)\n",
    "    stdev_vec = np.tile(stdev, (len(mfcc), 1)).T\n",
    "    output = mean_subtracted / (stdev_vec)\n",
    "    output = np.delete(output, 0)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileProcessor(F, trainingSize):\n",
    "    x_train = []   #Training Data\n",
    "    x_test = []    #Testing Data \n",
    "    y_train = []   #Training Classifiers\n",
    "    y_test = []    #Testing Classifiers\n",
    "    for file in F:    #F is a vector of file names\n",
    "        x = [] #data\n",
    "        y = [] #classifier\n",
    "        tree = ET.parse(file) #Create XML file tree\n",
    "        root = tree.getroot() #Get root of tree... In our case root is 'feature_vector_file'\n",
    "        for child in root:    #Loop through subelements of root\n",
    "            if child.tag == 'data_set':      #if subelement is 'data_set', perform:\n",
    "                fileID = child.find('data_set_id').text   #'data_set_id' contains the classifer (i.e the composer name)\n",
    "                composerName = fileID.split('/')[-2]      #extract composer name from 'data_set_id'\n",
    "                y.append(composerName)                    #append composer name to classifier vector\n",
    "                FeatureList = []                          #empty array for List of Features, Features have a name and set of values\n",
    "                for feature in child.iter('feature'):  #Loop through 'data_set' subelement for features\n",
    "                    if feature.find('name').text in ['MFCC Overall Average']:#['Power Spectrum Overall Average', 'Spectral Flux Overall Average', 'Beat Histogram Overall Average', 'MFCC Overall Average']:\n",
    "                        v = []                                #create empty array to contain feature values\n",
    "                        name = feature.find('name').text      #extract name of feature\n",
    "                        vals = feature.findall('v')           #extract values of feature\n",
    "                        for i in vals:                    #loop through extracted values\n",
    "                            v.append(i.text)                  #append text of values to values array\n",
    "                        if name == 'MFCC Overall Average':\n",
    "                            v = MFCCnormalizer(v)\n",
    "                        feat = [name, v]                  #join feature name and values\n",
    "                        FeatureList.append(feat)          #append feature name and values to List of Features\n",
    "                x.append(FeatureList)           #append List of Features for midi file being analysed to the array of data\n",
    "        x_trainC, x_testC, y_trainC, y_testC = train_test_split(x, y,test_size=trainingSize) #split data into training and testing data\n",
    "        #the following is done to ensure that each composer has an equal ratio of music split\n",
    "        #into training and testing data... although the specific songs by each composer are chosen randomly\n",
    "        #each composer has an equal ratio of their music split into testing and training data\n",
    "        x_train = x_train + x_trainC    \n",
    "        x_test = x_test + x_testC\n",
    "        y_train = y_train + y_trainC\n",
    "        y_test = y_test + y_testC\n",
    "    training = [x_train, y_train]\n",
    "    testing = [x_test, y_test]\n",
    "    return(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureProcessor(data):\n",
    "    x_train = data[0]\n",
    "    y_train = data[1]\n",
    "    beethovenTrain = [x_train[i] for i in range(len(x_train)) if y_train[i]== 'Beethoven']\n",
    "    chopinTrain = [x_train[i] for i in range(len(x_train)) if y_train[i]== 'Schubert']\n",
    "    \n",
    "    prior_beethoven = len(beethovenTrain)/(len(beethovenTrain)+len(chopinTrain))\n",
    "    prior_chopin = len(chopinTrain)/(len(beethovenTrain)+len(chopinTrain))\n",
    "    \n",
    "    priors = [prior_beethoven, prior_chopin]\n",
    "    \n",
    "    dataTrain = [beethovenTrain, chopinTrain]\n",
    "    \n",
    "    for composerData in range(len(dataTrain)):\n",
    "        featureList = []\n",
    "        for feature in range(len(dataTrain[composerData][0])):\n",
    "            featureData = []\n",
    "            for value in range(len(dataTrain[composerData][0][feature][1])):\n",
    "                vals = []\n",
    "                for song in range(len(dataTrain[composerData])):\n",
    "                    vals.append(dataTrain[composerData][song][feature][1][value])\n",
    "                vals = np.array(vals).astype(np.float)\n",
    "                mean = np.mean(vals)\n",
    "                stdev = np.std(vals)\n",
    "                featureInfo = [mean, stdev]\n",
    "                featureData.append(featureInfo)\n",
    "            nameAndData = [dataTrain[composerData][0][feature][0], featureData]\n",
    "            featureList.append(nameAndData)\n",
    "        dataTrain[composerData] = featureList\n",
    "    return(dataTrain, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(x, mean, stdev):\n",
    "    like = (1/(2*math.pi*stdev)**(1/2))*(math.exp((-1/2)*(((x-mean)**2)/stdev)))\n",
    "    return like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayesTesting(trainingData, processedDataD, testingData):\n",
    "    processedData = processedDataD[0]\n",
    "    priors = processedDataD[1]\n",
    "    x_test = testingData[0]\n",
    "    y_test = testingData[1]\n",
    "    testProbabilities = []\n",
    "    for testSong in range(len(x_test)):\n",
    "        composerProbability = []\n",
    "        for composer in range(len(processedData)):\n",
    "            p = 0\n",
    "            for feature in range(len(x_test[testSong])):\n",
    "                for value in range(len(x_test[testSong][feature][1])):\n",
    "                    probability = likelihood(float(x_test[testSong][feature][1][value]), processedData[composer][feature][1][value][0], processedData[composer][feature][1][value][1])\n",
    "                    p = p + probability\n",
    "            if composer == 0:\n",
    "                prob = ['Beethoven', p]\n",
    "            else:\n",
    "                prob = ['Chopin', p]\n",
    "            composerProbability.append(prob)\n",
    "        testProbabilities.append(composerProbability)\n",
    "    prior_beethoven = priors[0]\n",
    "    prior_chopin = priors[1]\n",
    "    results = []\n",
    "    for song in range(len(testProbabilities)):\n",
    "        prob_beethoven = (testProbabilities[song][0][1]*prior_beethoven)/((testProbabilities[song][0][1]*prior_beethoven)+(testProbabilities[song][1][1]*prior_chopin))\n",
    "        prob_chopin = (testProbabilities[song][1][1]*prior_chopin)/((testProbabilities[song][0][1]*prior_beethoven)+(testProbabilities[song][1][1]*prior_chopin))\n",
    "        if prob_beethoven > prob_chopin:\n",
    "            results.append('Beethoven')\n",
    "        elif prob_beethoven < prob_chopin:\n",
    "            results.append('Chopin')\n",
    "    print('Result | True')\n",
    "    for i in range(len(y_test)):\n",
    "        print(results[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result | True\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Beethoven\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n",
      "Beethoven Schubert\n"
     ]
    }
   ],
   "source": [
    "v = ['beethovenValues.xml', 'schubertValues.xml']\n",
    "d = fileProcessor(v, 0.33)\n",
    "p = featureProcessor(d[0])\n",
    "NaiveBayesTesting(d[0], p, d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi bye\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
